Note:  You can set the email notification address on the line with the sbatch command as follows:-
       sbatch --mail-user=yourname@nsidc.org run_setup_year.sh yyyy Fxx

For multiple users, you can include a double-quoted, comma-separated list.
       
1.  Transfer all of the input files to summit, see notes, here:

    https://nsidc.org/confluence/display/PMESDR/Running+processing+on+summit

    for installation, and notes on transferring input files to summit.
    
    1a. Start a processing info file, e.g. /projects/moha2290/F16_info to
    keep track of notes on files and days processed.

2.  Take note of the start doy, end doy, total doys, total number
    of input files

    The required shell scripts are all in the

    $MEASURES-BYU/CETB_process/scripts/
    
    directory.  Add this to your $PATH, or set an ENV variable with
    this location.
    
    Note that for SMM/I and SSMIS, you will generate 42 setup
    files per day and 84 *.nc CETB data files per day

x.  Make the set of required directories needed.
    Run make_sensor_dirs.sh to just do them all

    cd /scratch/summit/$USER/
    make_sensor_dirs.sh $SENSOR

3.  cd to your location of $SENSOR_scripts
    Run nc_to_gsx.sh to create script in the scripts directory
    called gsx_lb_list_summit to convert input to gsx - arguments
    SSMIS-CSU and source (F18 e.g.)
    This takes about 3-4 minutes.

4.  Verify contents of generated file with head, tail and wc -l
    (should have same number of lines as files you are expecting
    to process)

5.  From $SENSOR_scripts, do

    sbatch --mail-user=yourname@nsidc.org run_gsx.sh Fxx $CONDAENV

    to create gsx files files and verify file count in xxx_GSX directory.
    Stderr  will be written to $SENSOR_scripts/output/ directory

    5a. Use squeue --user=$USER --long to monitor the queue activity

6.  From $SENSOR_scripts, do

    run daily_file_lists.sh to create list of input files for each day in series,
    these are stored in directory $SENSOR_lists, 1 file per day

    arguments are year, start_doy, end_doy, platform i.e. yyy ddd ddd Fxx

7.  run file_3day.sh to create list of input files for NS projections for each day in series
    	arguments are yyyy ddd ddd Fxx

8.  count to confirm the correct number of files created, 2 files
    per day for total number of days

9.  use grep to check for no "such" messages in files created -
    should only be in NS files for first and last doys - this is
    useful because it will also show if there are missing days of
    files

10. use sed to edit files to remove missing file messages
    	grep -l such ../Fxx_lists/* | xargs sed -i '/such/d'

11. From $SENSOR_scripts, do

    SSMIS_make.sh to create lists for make process

    for each year to process.  Arguments are year, start_doy, end_doy, src and
    path to summit_set_pmesdr_environment.sh script, which will depend
    on where you installed the system, usually $MEASURES-BYU/src/prod

    N.B. If you run this for multiple years, they will each
    append to the end of the $SENSOR_make_list file, currently
    there is no check for this file existing to begin with, so be
    careful if you get interrupted to clean it up before
    restarting from the first year.

12. count number of lines in input file to run_make, there should
    be 4 * number of days to process

13. From $SENSOR_scripts, do

    sbatch --mail-user=yourname@nsidc.org run_make.sh Fxx /path/to/set_env

    takes 2 arguments, the sensor and the 
    path to summit_set_pmesdr_environment.sh script, which will depend
    on where you installed the system, usually $MEASURES-BYU/src/prod

    This generates make files for setup process - files
    are stored in $SENSOR_make

From this point processing should proceed by year.  (Note that 1
year of SSMIS setup files is around 15 - 17 TB, so plan
accordingly.)

14. from $SENSOR_scripts dir, execute SSMIS_setup_year.sh
    with arguments of 4-digit year
    platform and path to summit_set_pmesdr_environement.sh script,
    to generate list of commands for loadbalancer

15. sbatch run_setup_year.sh to run 1 year -- only run 3 of these at one time:

    sbatch --mail-user=yourname@nsidc.org run_setup_year.sh YYYY Fxx /path/to/set_env

    Use "grep -i error" and "grep -i warning" on batch stderr output.
    Setup currently reports warnings about precompute files, this can be ignored.
    (We will be removing this misleading message.)
    
16. after completion, from $SENSOR_scripts dir, execute SSMIS_sir_year.sh to create list of
    sir commands for loadbalancer:

    SSMIS_sir_year.sh yyyy platform /path/to/set_env

17. sbatch run_sir_year.sh:

    sbatch --mail-user=yourname@nsidc.org run_sir_year.sh YYYY Fxx /path/to/set_env

    Use "grep -i error" and "grep -i warning" on batch stderr output.
    A successful sir batch should return no warnings or errors.
    
18. Once setup has been completed for a given year (but before sir is completed),
    you can set up a batch job that will remove the very large setup files, contingent on the     success of the sbatch that runs the sir commands.  To do this:
    
    From $SENSOR_scripts dir, execute SSMIS_setup_rm_year.sh
    with arguments of 4-digit year, platform and
    path to summit_set_pmesdr_environement.sh script,
    to create list of setup files to remove:

    SSMIS_setup_rm_year.sh yyyy platform /path/to/set_env

19. And then the sbatch job to remove the input files can be
    scheduled to only run if the sir job completes successfully.
    So you do sbatch run_setup_rm_year.sh, with dependency as
    follows:-

    sbatch --dependency=afterok:jobid_from_sir run_setup_rm_year.sh yyyy Fxx /path/to/set_env

20. at this point I usually edit the list of sir commands using
    sed to create the lists for further years

21. also edit the list of setup rm commands with sed to change
    year

22. Now you can proceed to chain setup, sir and setup_rm jobs
    with dependencies.


Once the processing has completed, you have to move the files
around and run the premet and spatial file generation

1.  Run the script file_create_dirs.sh - takes the platform and
    start/stop yyyymm as arguments

2.  Execute the script file_move_dir.sh (yyyy, Fxx and
    type as arguments) to create a list of mv commands by year
    cat all of the output files into moving_files_all

    for y in $(seq 1990 1997) ; do file_move_dir.sh $y $SRC SSMI ; done

3.  sbatch run_mv_files.sh to move the files - two arguments are
    Fxx (src) and path to summit_set_pmesdr_environement.sh script,

    sbatch --mail-user=yourname@nsidc.org run_mv_files.sh Fxx /path/to/set_env

4.  From $SENSOR_scripts dir, execute premetandspatial.sh
    with Fxx and path to summit_set_pmesdr_environement.sh script,
    as argument to generate list for loadbalancer:

    premetandspatial.sh Fxx /path/to/set_env

5.  Make sure your conda env CONDAENV has cetbtools installed, and
    sbatch run_premet_cetb.sh, second arg is CONDAENV:

    sbatch --mail-user=yourname@nsidc.org run_premet_cetb.sh Fxx CONDAENV
    
6.  run manifest.sh to create the manifest list for OPS

7.  Make sure that group read permissions are set recursively on
    Fxx_sir, and on /scratch/summit/$USER/ and on the
    Fxx_manifest file for the DAAC operator's user name.  At this
    point, the operators are not in any shared groups on summit, so
    this requires setting dirs to o+r, o+x and all files to o+r.

    At some point, my workflow created .tramp_history files,
    remove these so they don't get transferred over.

    Send email to DAAC about new batch of processing, wait for
    DAAC confirmation of ingest before deleting Fxx_sir.  Save
    the Fxx_scripts directory to someplace off scratch.